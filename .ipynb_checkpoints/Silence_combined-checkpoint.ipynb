{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-35-5dcff40a8f64>, line 19)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-35-5dcff40a8f64>\"\u001b[0;36m, line \u001b[0;32m19\u001b[0m\n\u001b[0;31m    for subcarrier in range(90)\u001b[0m\n\u001b[0m                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io as sio\n",
    "import math\n",
    "import numpy as np\n",
    "from scipy.signal import butter, lfilter, filtfilt, medfilt\n",
    "\n",
    "matlab_file = './Sushant_CSI/1m/backward_converted/log_2.mat'\n",
    "plt.rcParams['figure.dpi'] = 275\n",
    "\n",
    "original = [] # Dataset with trimmed csi streams, removed silence\n",
    "selected_subcarrier = 0\n",
    "\n",
    "mat_contents = sio.loadmat(matlab_file)\n",
    "antenna = mat_contents['M']\n",
    "\n",
    "# -- APPLY BUTTERWORTH FILTER --\n",
    "for subcarrier in range(90):\n",
    "    fs = 0.05        # Sampling Frequency\n",
    "    flt_ord = 12     # Filter order number\n",
    "\n",
    "    # Create list of CSI stream of one subcarrier\n",
    "    x_axis = []\n",
    "    y_axis = []\n",
    "    for i in range(0, len(antenna)):\n",
    "        x_axis.append(i)\n",
    "        y_axis.append(antenna[i][subcarrier])\n",
    "\n",
    "    flt_ord = 12\n",
    "    b, a = butter(flt_ord, 0.05, 'lowpass', analog=False)\n",
    "    output = filtfilt(b, a, y_axis)\n",
    "    original.append(output)\n",
    "\n",
    "\n",
    "# -- SILENCE REMOVAL ---\n",
    "# Separate the CSI data into a sequence of frames\n",
    "num_packets = len(original[selected_subcarrier])\n",
    "data_per_frame = 50    # 50 ms is the number of data(frequency) in each frame.\n",
    "total_num_frames = math.floor(num_packets/data_per_frame) \n",
    "frames = []\n",
    "for frame in range(int(total_num_frames)):\n",
    "    cur_index = data_per_frame*frame\n",
    "    seq = output[cur_index : cur_index+data_per_frame]\n",
    "    frames.append(seq)\n",
    "\n",
    "\n",
    "# Calculate energy for each frame\n",
    "# Energy each frame = Average of the Squares for each frame\n",
    "energy = []\n",
    "for frame in frames:\n",
    "    sq_sum = sum(map(lambda x: x*x, frame))\n",
    "    e_frame = sq_sum/data_per_frame\n",
    "    energy.append(e_frame)\n",
    "\n",
    "# Apply median filter to energy values and take their log\n",
    "filt_energy = medfilt(energy)\n",
    "filt_energy = [math.log(e) for e in filt_energy]\n",
    "\n",
    "# Select contiguous block of frames s_mid for which energy > mean energy\n",
    "\n",
    "# Get indices for which energy > mean energy\n",
    "curr_index = 0\n",
    "high_energy_indices = []\n",
    "mean_energy = sum(filt_energy)/len(filt_energy)\n",
    "for energy in filt_energy:\n",
    "    if energy > mean_energy:\n",
    "        high_energy_indices.append(curr_index)\n",
    "    curr_index += 1;\n",
    "\n",
    "# Split the high energy indices into contigious subsequences\n",
    "curr_index = None\n",
    "inside_arr = []\n",
    "conti_arr = []\n",
    "for index in high_energy_indices:\n",
    "    if (curr_index is None):\n",
    "        inside_arr.append(index)\n",
    "    else:\n",
    "        if (curr_index == index-1):\n",
    "            inside_arr.append(index)\n",
    "        else:\n",
    "            conti_arr.append(inside_arr)\n",
    "            inside_arr = []\n",
    "            inside_arr.append(index)\n",
    "\n",
    "    curr_index = index\n",
    "\n",
    "if inside_arr:\n",
    "    conti_arr.append(inside_arr)\n",
    "\n",
    "# Select the longest contigious subsequence\n",
    "s_mid = max(conti_arr, key=len)\n",
    "\n",
    "# Get the corresponding CSI data frames h_mid for s_mid\n",
    "h_mid = []\n",
    "for index in s_mid:\n",
    "    h_mid.append(frames[index])\n",
    "\n",
    "    \n",
    "# Calculate the midpoint of the region h_mid which has maximum\n",
    "# deviations from the average in h_mid\n",
    "high_energy_csi = [item for sublist in h_mid for item in sublist]\n",
    "avg_energy = sum(high_energy_csi)/len(high_energy_csi)\n",
    "\n",
    "max_dev = 0\n",
    "max_dev_index = 0\n",
    "for index, energy in enumerate(high_energy_csi):\n",
    "    deviation = abs(avg_energy - energy)\n",
    "    if deviation > max_dev:\n",
    "        max_dev = deviation\n",
    "        max_dev_index = index\n",
    "\n",
    "m = s_mid[0] * data_per_frame + max_dev_index \n",
    "\n",
    "# Set start_point to mâˆ’T/2, where T is total duration\n",
    "start_point = math.floor(m - (num_packets)/2)\n",
    "start_point = max(start_point, 0)\n",
    "\n",
    "# Set end_point to m+T/2, where T is total duration\n",
    "end_point = math.floor(m + (num_packets)/2)\n",
    "end_point = min(end_point, num_packets)\n",
    "\n",
    "# Get original data with silence removed\n",
    "trimmed_output = original[selected_subcarrier][start_point:end_point]\n",
    "\n",
    "silenced_data = [] # Contains the original csi data for which each subcarrier is trimmed from best_start to best_end\n",
    "for untrimmed in original:\n",
    "    new_csi = untrimmed[best_start:best_end]\n",
    "    silenced_data.append(new_csi)\n",
    "\n",
    "\n",
    "# Plot the silenced CSI data\n",
    "for silenced in silenced_data:\n",
    "    plt.plot(silenced)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Amplitude')\n",
    "    plt.title('Filtered CSI stream (Ord 12) - Silence removed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for untrimmed in original:\n",
    "#     plt.plot(untrimmed)\n",
    "#     plt.xlim(left=0, right=num_packets)\n",
    "#     plt.xlabel('Time')\n",
    "#     plt.ylabel('Amplitude')\n",
    "#     plt.title('Filtered CSI stream (Ord 12) - Silence removed') \n",
    "\n",
    "# SIGNAL SEPARATION\n",
    "# Now start Signal separation\n",
    "# import mlpy\n",
    "\n",
    "# x = H[0]\n",
    "# coef, freqs=pywt.cwt(x,np.arange(1,10),'morl')\n",
    "# plt.plot(coef)\n",
    "# plt.specgram(freqs, Fs=10, cmap='jet', NFFT=5, noverlap=4, scale_by_freq=True,)# xextent=(0,1.627))\n",
    "\n",
    "# plt.matshow(coef)\n",
    "# plt.show() \n",
    "\n",
    "# dt = 1\n",
    "# # scales = [6,7,8,9]\n",
    "# # print(H[0])\n",
    "# # # n = int()\n",
    "# scales = mlpy.wavelet.autoscales(N=H[0].shape[0], dt=dt, dj=0.1, wf='morlet', p=2)\n",
    "# # print(scales.shape)\n",
    "# # print(type(scales))\n",
    "# # new_scales = scales.view(-1)\n",
    "# # print(new_scales)\n",
    "# X = mlpy.wavelet.cwt(H[0], dt, scales, wf='morlet', p=2)\n",
    "# print(scales)\n",
    "# # print(X)\n",
    "\n",
    "# # fig = plt.figure(1)\n",
    "# # ax1 = plt.subplot(2,1,1)\n",
    "# # p1 = ax1.plot(H[0])\n",
    "# # ax1.autoscale_view(tight=True)\n",
    "# # ax2 = plt.subplot(2,1,2)\n",
    "# # p2 = ax2.imshow(np.abs(X), interpolation='nearest')\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "# #print(np.abs(X))\n",
    "# plt.imshow(np.abs(X))\n",
    "# # plt.xlim(left=3500, right=4000)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import kurtosis, skew\n",
    "#from skrebate import ReliefF\n",
    "#from sklearn.pipeline import make_pipeline\n",
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "#from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "# Feature extraction shit here.\n",
    "num_packets = len(silenced_data[0])\n",
    "data_per_window = 100\n",
    "total_num_windows = math.floor(num_packets/data_per_window) \n",
    "\n",
    "# Split data into windows of 100 packets / 0.1 seconds\n",
    "windows = []\n",
    "for window in range(int(total_num_windows)):\n",
    "    cur_index = data_per_window*window\n",
    "    seq = silenced_data[0][cur_index : cur_index+data_per_window]\n",
    "    \n",
    "    windows.append(seq)\n",
    "\n",
    "#print(windows)\n",
    "\n",
    "features = []\n",
    "\n",
    "def getZeroCrossingRate(arr):\n",
    "        my_array = np.array(arr)\n",
    "        #print(my_array)\n",
    "        return float(\"{0:.2f}\".format((((my_array[:-1] * my_array[1:]) < 0).sum())/len(arr)))\n",
    "    \n",
    "def getMeanCrossingRate(arr):\n",
    "    #print(arr)\n",
    "    return getZeroCrossingRate(np.array(arr) - np.mean(arr))\n",
    "\n",
    "# For each window, gather its features\n",
    "for each_window in windows:\n",
    "    # time domain features\n",
    "    \n",
    "    weight_each_window = []\n",
    "    \n",
    "    mean = sum(each_window) / len(each_window)\n",
    "    max_val = max(each_window)\n",
    "    min_val = min(each_window)\n",
    "    skewness = skew(each_window)\n",
    "    kurtosis_val = kurtosis(each_window)\n",
    "    variance = np.var(each_window)\n",
    "    \n",
    "    # !!! MEAN crossing rate shit doesnt work !!!\n",
    "    mean_crossing_rate = getMeanCrossingRate(each_window)\n",
    "    \n",
    "    weight_each_window.append(mean)\n",
    "    weight_each_window.append(max_val)\n",
    "    weight_each_window.append(min_val)\n",
    "    weight_each_window.append(skewness)\n",
    "    weight_each_window.append(kurtosis_val)\n",
    "    weight_each_window.append(variance)\n",
    "    weight_each_window.append(mean_crossing_rate)\n",
    "    \n",
    "    features.append(weight_each_window)\n",
    "    \n",
    "# # print(mean)\n",
    "# # print(max_val)\n",
    "# # print(min_val)\n",
    "# # print(skewness)\n",
    "# # print(kurtosis_val)\n",
    "# # print(variance)\n",
    "# # print(mean_crossing_rate)\n",
    "\n",
    "# # print(features)\n",
    "# label_len = len(features)\n",
    "# labels = np.zeros(label_len)\n",
    "\n",
    "# print(labels)\n",
    "# clf = make_pipeline(ReliefF(n_features_to_select=2, n_neighbors=100),\n",
    "#                     RandomForestClassifier(n_estimators=100))\n",
    "\n",
    "\n",
    "# print(clf)\n",
    "# print(np.mean(cross_val_score(clf, features, labels)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Doing the classifying using features above\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
